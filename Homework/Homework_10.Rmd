---
title: "Homework 10"
author: "Mia Murphy"
date: "4/29/2021"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Using a for loop, write a function to calculate the number of zeroes in a numeric vector. Before entering the loop, set up a counter variable counter <- 0. Inside the loop, add 1 to counter each time you have a zero in the matrix. Finally, use return(counter) for the output.

```{r}
library(dplyr)


abc <- rbinom(15, 4, 0.1)
my_loop <- function(a=abc) {
counter <- 0
 for(i in seq_along(a)) {
  if(a[i] == 0) {
    counter <- counter + 1
  }}   
    return(counter)
}

print(my_loop())

abc
```

### Use subsetting instead of a loop to rewrite the function as a single line of code.

```{r}

newabc <- rbinom(15, 4, 0.1)

counting <- function(a=newabc) {
  checkzero <- length(a[a==0])
  return(checkzero)
}
  
counting()
newabc
```

### Write a function that takes as input two integers representing the number of rows and columns in a matrix. The output is a matrix of these dimensions in which each element is the product of the row number x the column number.



```{r}
matrixtime <- function(rows = 3, cols = 3) {
  
  my_matrix <- matrix(data = NA, nrow = rows, ncol = cols)
  for(i in 1:rows) {
  for(j in 1:cols) {
    my_matrix[i,j] <-  i * j 
  } 
} 
    return(my_matrix) 
}

matrixtime()
```



### Use the code from the April 8th lecture (Randomization Tests) to design and conduct a randomization test for some of your own data. You will need to modify the functions that read in the data, calculate the metric, and randomize the data. Once those are set up, the program should run correctly calling your new functions. Also, to make your analysis fully repeatable, make sure you set the random number seed at the beginning (use either set.seed() in base R, or char2seed in the TeachingDemos package



```{r}
library(TeachingDemos)
library(ggplot2)
set.seed(1000)

#Fake data from before

preybison <- rnorm(n=20, mean=2500, sd=500) 
filler <- rnorm(n=20, sd=3) 
wolfpack <- 2 + preybison * 0.01 + filler

newdata <- function(z=NULL) {
  if(is.null(z)) {
    x_obs <- preybison
    y_obs <- wolfpack
    df <- data.frame(ID=seq_along(x_obs),
                     x_obs,
                     y_obs)}
  return(df)
  
}

newdata()

```



```{r}
#metric

themetric <- function(z = NULL) {
  if(is.null(z)){
    xobs <- preybison
    yobs <- wolfpack
    z <- data.frame(ID=seq_along(xobs),
                    xobs,
                    yobs)}
  . <-lm(z[,3] ~ z[,2])
  . <- summary(.)
  . <- .$coefficients[2,1]
  slope <- .
  
return(slope)
}
themetric()
```



```{r}
#random

randomdat <- function(z=NULL){
  if(is.null(z)){
    xobs <- preybison
    yobs <- wolfpack
    z <- data.frame(ID=seq_along(xobs),
                    xobs,
                    yobs)}
  z[,3] <- sample(z[,3])
  return(z)
}

randomdat()

```



```{r}
#p value
p_val1 <- function(z=NULL) {
  if(is.null(z)){
    xobs <- preybison
    yobs <- wolfpack
    z <- data.frame(ID=seq_along(xobs),
                    xobs,
                    yobs)}
  z[,3] <- sample(z[,3])
  . <- lm(z[,3] ~ z[,2])
  . <- summary(.)
  . <- .$coefficients[2,4]
  p_val2 <- .
  return(p_val2)
}

p_val1()

```



```{r}
#lower and upper p value lupval

lupval <- function(z = NULL) {
  if(is.null(z)){
    z <- list(xobs = runif(1), x_sim = runif(1000)) }
  lowerp <- mean(z[[2]] <= z[[1]])
  upperp <- mean(z[[2]] >= z[[1]])
  
  return(c(pl = lowerp,  pu = upperp))
}

lupval()
```

```{r, warning = FALSE}
library(ggplot2)

plotrtest <- function(z = NULL) {
  if(is.null(z)){
    z <- list(rnorm(1), rnorm(1000)) }
  
  df <- data.frame(ID = seq_along(z[[2]]), x_sim = z[[2]])
  p_1 <- ggplot(data = df, mapping = aes(x = x_sim))
  p_1 + geom_histogram(mapping=aes(fill = I("pink"), color = I("black"))) +
    geom_vline(aes(xintercept = z[[1]], col = "purple")) 
  
}

plotrtest()
```


```{r, warning=FALSE}
set.seed(100)

n_sim <- 10000

x_sim <- rep(NA, n_sim) 

df <- newdata()

x_obs <- themetric(df)

for (i in seq_len(n_sim)) {
  x_sim[i] <- themetric(randomdat(df))
}

slopes <- list(x_obs, x_sim)

lupval(z = slopes)

```

```{r}

plotrtest(slopes)

```



```{r}

checkfinal <- lm(x_obs~y_obs, data=df)

```


```{r}

summary(checkfinal)

```



### For comparison, calculate in R the standard statistical analysis you would use with these data. How does the p-value compare for the standard test versus the p value you estimated from your randomization test? If the p values seem very different, run the program again with a different starting seed (and/or increase the number of replications in your randomization test). If there are persistent differences in the p value of the standard test versus your randomization, what do you think is responsible for this difference?


The p values change drastically. The p values from the original fake data simulation that I created are closer to 1 and make a bit more sense since everything was tailored to mimic a study conclusion. Not sure that I did anything correctly here, so it could be an error that I made that is causing some confusion on what I am seeing.

